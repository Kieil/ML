{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNie47FwvJlhQ6S5HGkHsEK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEnMeiTrVzgT",
        "outputId": "d2d1451a-d0df-4235-da68-c33213a0467f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "#!pip install pandas\n",
        "#!pip install seaborn\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import read_csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "test= pd.read_csv('test.csv')\n",
        "train= pd.read_csv('train.csv')\n",
        "\n",
        "all_data = pd.concat([train, test])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "HEgMempNV_5f",
        "outputId": "f13b7668-83f1-4000-d94b-5a1048e61eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bf371b3e48ad>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['engine'] = all_data['engine'].str.replace('Liter', 'L')\n",
        "\n",
        "def extract_data_from_engine(all_data):\n",
        "    all_data=all_data.copy()\n",
        "    all_data['horsepower'] = all_data['engine'].str.extract(r'(\\d+\\.\\d+)(?=HP)').astype(float)\n",
        "    all_data['engine_size'] = all_data['engine'].str.extract(r'(\\d+\\.\\d+)(?=L)' or '(\\d+\\.\\d+)(?= Liter)' ).astype(float)\n",
        "    all_data['cylinders'] = all_data['engine'].str.extract(r'(\\d+)\\s(Cylinder|V\\d+|Straight)')[0].astype(float)\n",
        "    all_data['eletric_vol'] = all_data['engine'].str.extract(r'(\\d+)V')[0].astype(float)\n",
        "#filling empty in fuel column filling empty values of fuel type and changing eletric to hybrid\n",
        "    all_data['engine_extract'] = all_data['engine'].str.extract(r'(Gasoline|Diesel|Flex Fuel|Hybrid|Plug-In Hybrid|Electric)')[0]\n",
        "    all_data.loc[(all_data['engine'] == '-') & (all_data['accident'].isnull()) & (all_data['milage']>2000), 'accident'] = 'At least 1 accident or damage reported' #Added requested condition\n",
        "    all_data.loc[all_data['engine'].str.contains(\"Twin Turbo\"), 'twin_turbo'] = True\n",
        "    all_data['twin_turbo'] = all_data['twin_turbo'].fillna(False)\n",
        "    return all_data\n",
        "all_data = extract_data_from_engine(all_data)"
      ],
      "metadata": {
        "id": "niduZveTT0CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Fill missing values in 'fuel_type' with corresponding values from 'engine_extract'\n",
        "all_data['fuel_type'] = all_data['fuel_type'].fillna(all_data['engine_extract'])"
      ],
      "metadata": {
        "id": "xnnr0gJbyfpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_year = 2024\n",
        "all_data['Vehicle_Age'] = current_year - all_data['model_year']\n",
        "all_data['Mileage_per_Year'] = all_data['milage'] / (all_data['Vehicle_Age'] + 10e-5)\n",
        "all_data['brand'] = all_data['brand'].str.lower()\n",
        "all_data['ext_col'] = all_data['ext_col'].str.lower()\n",
        "all_data['transmission'] = all_data['transmission'].str.lower()\n",
        "all_data['int_col'] = all_data['int_col'].str.lower()"
      ],
      "metadata": {
        "id": "kGQWDrxdVTas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['antique'] = all_data['Vehicle_Age'].apply(lambda x: 1 if x < 20 else 0)\n",
        "\n",
        "all_data['new'] = all_data['milage'].apply(lambda x: 1 if x < 200 else 0)"
      ],
      "metadata": {
        "id": "w1CY6U4lyDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luxury_brands = ['lamborghini', 'bugatti', 'rolls-royce','bentley', 'mclaren', 'ferrari', 'aston','rivian']\n",
        "all_data['luxury'] = all_data['brand'].apply(lambda x: 1 if x in luxury_brands else 0)\n"
      ],
      "metadata": {
        "id": "1evRIdi21IBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_from_col_ext(all_data):\n",
        "    all_data = all_data.copy()\n",
        "    counts = all_data['ext_col'].value_counts()\n",
        "    all_data['ext_col_u'] = all_data['ext_col'].apply(lambda x: 1 if counts[x] < 100 else 0)\n",
        "    return all_data\n",
        "all_data = extract_data_from_col_ext(all_data)\n"
      ],
      "metadata": {
        "id": "tH7s-CK9j37f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_from_col_int(all_data):\n",
        "    counts = all_data['int_col'].value_counts()\n",
        "    all_data['int_col_u'] = all_data['int_col'].apply(lambda x: 1 if counts[x] < 100 else 0)\n",
        "    return all_data\n",
        "all_data = extract_data_from_col_int(all_data)"
      ],
      "metadata": {
        "id": "ujsUjr9IlbsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['transmission_auto'] = all_data['transmission'].apply(lambda x: 1 if 'automatic' in x or 'a/t' in x else 0)\n"
      ],
      "metadata": {
        "id": "Ea8nvObt9Rk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average horsepower for each brand\n",
        "avg_horsepower_by_brand = all_data.groupby('brand')['horsepower'].mean()\n",
        "\n",
        "# Fill nulls in horsepower with the average horsepower for the corresponding brand\n",
        "all_data['horsepower'] = all_data.apply(\n",
        "    lambda row: avg_horsepower_by_brand[row['brand']] if pd.isnull(row['horsepower']) else row['horsepower'],\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "sSwzovB7C0VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['accidents_count'] = all_data.groupby('brand')['accident'].transform('count')"
      ],
      "metadata": {
        "id": "N56HZSMEKBaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "object_cols = ['brand','model', 'fuel_type', 'int_col','ext_col', 'accident', 'clean_title','transmission']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in object_cols:\n",
        "    # Convert all values to strings before fitting the encoder\n",
        "    all_values = pd.concat([all_data[col]]).astype(str).unique()\n",
        "    label_encoder.fit(all_values)\n",
        "\n",
        "    all_data[col] = label_encoder.transform(all_data[col].astype(str))"
      ],
      "metadata": {
        "id": "wyXLWT3cIfl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = all_data.dropna(subset=['price'])\n",
        "test = all_data[all_data['price'].isnull()]\n",
        "\n",
        "test1 = test.copy()\n",
        "test1 = test['id']\n",
        "\n",
        "\n",
        "train = train.drop(['id','engine','engine_extract'], axis=1) # Dropping columns from the train DataFrame\n",
        "test = test.drop(['id','engine','price','engine_extract'], axis=1) # Dropping columns from the test DataFrame\n",
        "\n",
        "# train1 = train.drop(['id', 'model','engine','price','engine_extract','int_col','ext_col','ext_col_u','int_col_u'],axis =1)\n",
        "# test1 = test.drop(['id','model','engine','engine_extract','int_col','ext_col','ext_col_u','int_col_u'], axis = 1)"
      ],
      "metadata": {
        "id": "xR5GnW1jq302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "ijktfjzNEaW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    'n_estimators': 1225,\n",
        "    'num_leaves': 137,\n",
        "    'max_depth': 17,\n",
        "    'cat_smooth': 96,\n",
        "    'learning_rate': 0.02,\n",
        "    'subsample': 0.9082095260228584,\n",
        "    'colsample_bytree': 0.6165900236226695,\n",
        "    'min_split_gain': 0.0308677316309982,\n",
        "    'min_child_weight': 32,\n",
        "    'lambda_l2': 1.7319600391087514e-07,\n",
        "    'lambda_l1': 8.761594422544116e-07,\n",
        "    'max_bin': 749,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'random_state': 42,\n",
        "    'boosting_type': 'gbdt',\n",
        "}\n",
        "\n",
        "## {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 31} - optional"
      ],
      "metadata": {
        "id": "kgbWfxG6kK3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_model = lgb.LGBMRegressor(**best_params)\n",
        "\n",
        "callbacks = [\n",
        "    lgb.early_stopping(stopping_rounds=500),\n",
        "    lgb.log_evaluation(100)\n",
        "]"
      ],
      "metadata": {
        "id": "-o4B2WX_kUAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "Lz2HZo7CEJve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = train.drop(['price'], axis=1)\n",
        "fit1 = train['price']"
      ],
      "metadata": {
        "id": "RYS7T-x4FDYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_model.fit(\n",
        "    train1, fit1,\n",
        "    eval_set=[(train1, fit1)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=callbacks\n",
        " )"
      ],
      "metadata": {
        "id": "fXvT3pZzkYwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred1 = lgb_model.predict(test)\n",
        "#pred1 = np.clip(pred1, 2000, None)"
      ],
      "metadata": {
        "id": "BQpoEfABk5GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred2 = lgb_model.predict(train1)"
      ],
      "metadata": {
        "id": "HGu6ImKjmM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(fit1, label='Actual Prices', color='blue', fill=True)\n",
        "sns.kdeplot(pred2, label='Predicted Prices', color='orange', fill=True)\n",
        "plt.xlabel(\"Prices\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Distribution of Actual vs. Predicted Prices\")\n",
        "plt.legend()\n",
        "plt.xlim(0, 80000)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KYg4zrXM6ye8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "data = {'model': lgb_model}\n",
        "with open('saved_steps.pkl', 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ],
      "metadata": {
        "id": "0wZu-_VgYJHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open ('saved_steps.pkl', 'rb') as file:\n",
        "#     data = pickle.load(file)\n",
        "\n",
        "#   regressor_loaded = data['model']\n",
        "#   pred1 = regressor_loaded.predict(train1)"
      ],
      "metadata": {
        "id": "XEi4CK2LYjpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'id': test1,\n",
        "    'price': pd.Series(pred1).astype(int)  # Convert predictions to integers\n",
        "})\n",
        "submission.to_csv('submission_car1.csv', index=False)"
      ],
      "metadata": {
        "id": "KFW08H8Q-zG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = submission.sort_values(by='price', ascending=True)\n",
        "submission"
      ],
      "metadata": {
        "id": "74Bpi9TZ_e-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
